{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "In this homework you are going to fit and compare linear models on how well they predict new data. You will need to install (at least) python, jupyter, matplotlib, and numpy to do this assignment. Installing anaconda is a quick way to get all of those things.\n",
    "\n",
    "There are 2 problems worth a total of 30 points. The description for each problem will tell you how many points each part is worth.\n",
    "\n",
    "You should not need to edit the boilerplate code in this notebook, but wherever you see `## YOUR CODE HERE ##`, you should replace that with your own code (obviously).\n",
    "\n",
    "To turn in your homework, email your finished `.ipynb` file to `huth@cs.utexas.edu`. This homework is due on 11/14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - Cross-validation (10 pts)\n",
    "\n",
    "Cross-validation is the method that we can use to choose hyperparameters, such as the ridge penalty $\\lambda$ in ridge regression. It is an empirical technique that is incredibly effective, even if the data does not exactly fit the assumptions of your model (e.g. if the errors are not gaussian, etc.). You can read more about cross-validation [here](https://en.wikipedia.org/wiki/Cross-validation_(statistics%29).\n",
    "\n",
    "As a basic scheme, imagine that you have data that is split into two parts: training and test. Let's suppose you try a bunch of different ridge parameters by fitting models using the training data. If you check how well each model predicts the training data (that you used to generate the weights), then you'll get a bad answer: most likely the smallest ridge parameter will work best (you saw this in the last homework). If you check how well each model predicts the test data, then you're cheating: you're touching the test data before you're done model fitting. **Don't cheat by fitting hyperparameters using the test data!!** So obviously we need another way to check the ridge parameters.\n",
    "\n",
    "Now suppose that you further split the training dataset into two parts: the \"training\" set (yes it's the same name, get over it), and a \"validation\" set. Now you can train your models using the training data, check how well they work using the validation dataset, and use those numbers to pick your hyperparameter. Then, finally, you can use the whole training dataset and the best hyperparameter to fit a model that you'll use to try to predict the test data (which you haven't touched until now). No cheating! This is cross-validation.\n",
    "\n",
    "In this problem, you are going to use three different cross-validation methods to select $\\lambda$ for ridge regression.\n",
    "\n",
    "**BIG HINT**: The `np.delete` function is your best friend for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data!\n",
    "p1a_file = np.load('homework_2_p1a_data.npz')\n",
    "x = p1a_file['x']\n",
    "y = p1a_file['y']\n",
    "x_test = p1a_file['x_test']\n",
    "y_test = p1a_file['y_test']\n",
    "\n",
    "n_training, n_features = x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Leave-one-out (LOO) cross-validation (3 pts)\n",
    "\n",
    "This is perhaps the simplest cross-validation method. Select one datapoint from the training set to be the validation set, then train on the other $n-1$ datapoints. Test on the remaining datapoint. Repeat this process for each datapoint, and then average the results.\n",
    "\n",
    "This works very well when the datapoints are independent, which is going to be the case in this first dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-3, 5, 10)\n",
    "val_ses = np.zeros((n_training, len(lambdas)))\n",
    "\n",
    "def ridge(x, y, lam):\n",
    "    \"\"\"This function does ridge regression with the stimuli x and responses y with\n",
    "    ridge parameter lam (short for lambda). It returns the weights.\n",
    "    This is definitely not the most efficient way to do this, but it's fine for now.\n",
    "    \"\"\"\n",
    "    n_features = x.shape[1]\n",
    "    beta_ridge = np.linalg.inv(x.T.dot(x) + lam * np.eye(n_features)).dot(x.T).dot(y)\n",
    "    return beta_ridge\n",
    "\n",
    "for t in range(n_training):\n",
    "    # split the training dataset into two parts: one with only point t,\n",
    "    # and one with all the other datapoints\n",
    "    x_trn = ## YOUR CODE HERE ##\n",
    "    y_trn = ## YOUR CODE HERE ##\n",
    "    \n",
    "    x_val = ## YOUR CODE HERE ##\n",
    "    y_val = ## YOUR CODE HERE ##\n",
    "    \n",
    "    for ii in range(len(lambdas)):\n",
    "        # fit model using x_trn & predict y_hal\n",
    "        y_val_hat = ## YOUR CODE HERE ##\n",
    "        \n",
    "        # store squared error in val_mses\n",
    "        val_ses[t,ii] = ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the mean squared error with each lambda, averaged across validation datapoints\n",
    "\n",
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose the best lambda (i.e. the one with the least validation error), print it\n",
    "best_lambda = ## YOUR CODE HERE ##\n",
    "print(\"Best lambda:\", best_lambda)\n",
    "\n",
    "# Fit a model using the whole training set and the best lambda\n",
    "beta_hat = ## YOUR CODE HERE ##\n",
    "\n",
    "# Use that model to predict y_test\n",
    "y_test_hat = ## YOUR CODE HERE ##\n",
    "\n",
    "# Compute the MSE on the test dataset, print it\n",
    "test_mse = ## YOUR CODE HERE ##\n",
    "\n",
    "print(\"Test MSE:\", test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) $k$-fold cross-validation (3 pts)\n",
    "\n",
    "You may have noticed that LOO CV is quite slow. That's because it needs to fit one model per datapoint. If you have a lot of datapoint, that's just not gonna work! Further, as mentioned above, LOO only works well when your datapoints are independent. Suppose you're recording fMRI data. You know that the underlying BOLD signal is very low frequency--meaning that the individual datapoints are anything but independent.\n",
    "\n",
    "Another option is $k$-fold cross-validation. In this scheme, you split the training dataset into $k$ parts. Then you use $k-1$ of the parts to train the model, and use the $k$'th part as your validation dataset. Repeat this, holding out each part in turn. This means you only need to fit $k$ sets of models, instead of one for each datapoint.\n",
    "\n",
    "If your entire training dataset has $n$ datapoints, each \"fold\" should contain $\\frac{n}{k}$ datapoints, and each datapoint should only be in one fold. Assuming that $n/k$ is an integer, a nice way to do this is to break up the training dataset into $k$ contiguous parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 6 # let's do 6 folds\n",
    "n_per_fold = n_training / k # number of datapoints per fold\n",
    "\n",
    "lambdas = np.logspace(-3, 5, 10)\n",
    "val_mses = np.zeros((k, len(lambdas)))\n",
    "\n",
    "\n",
    "for fold in range(k):\n",
    "    # split the training dataset into two parts: one with only the points in fold \"fold\"\n",
    "    # and one with all the other datapoints\n",
    "    \n",
    "    ## YOUR CODE HERE ##\n",
    "    \n",
    "    x_trn = ## YOUR CODE HERE ##\n",
    "    y_trn = ## YOUR CODE HERE ##\n",
    "    \n",
    "    x_val = ## YOUR CODE HERE ##\n",
    "    y_val = ## YOUR CODE HERE ##\n",
    "    \n",
    "    for ii in range(len(lambdas)):\n",
    "        # fit model using x_trn & predict y_val\n",
    "        y_val_hat = ## YOUR CODE HERE ##\n",
    "        \n",
    "        # store squared error in val_mses\n",
    "        val_mses[fold,ii] = ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the MSE for each lambda, averaged across the folds\n",
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose the best lambda, print it\n",
    "best_lambda = ## YOUR CODE HERE ##\n",
    "print(\"Best lambda:\", best_lambda)\n",
    "\n",
    "# Fit a model using the whole training set and the best lambda\n",
    "beta_hat = ## YOUR CODE HERE ##\n",
    "\n",
    "# Use that model to predict y_test\n",
    "y_test_hat = ## YOUR CODE HERE ##\n",
    "\n",
    "# Compute MSE on the test dataset, print it\n",
    "test_mse = ## YOUR CODE HERE ##\n",
    "\n",
    "print(\"Test MSE:\", test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Monte Carlo cross-validation (4 pts)\n",
    "\n",
    "One issue with $k$-fold CV is that the size of the validation set depends on the number of folds. If you want really stable estimates for your hyperparameter, you want to have a pretty large validation set, but also do a lot of folds. You can accomplish this by, on each iteration, randomly assigning some fraction of the training set to be the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_mc_iters = 50 # let's do 50 Monte Carlo iterations\n",
    "n_per_mc_iter = 50 # on each MC iteration, hold out 50 datapoints to be the validation set\n",
    "\n",
    "lambdas = np.logspace(-3, 5, 10)\n",
    "val_mses = np.zeros((n_training, len(lambdas)))\n",
    "\n",
    "\n",
    "for it in range(n_mc_iters):\n",
    "    # split the training dataset into two parts: one with a random selection of n_per_mc_iter points\n",
    "    # and one with all the other datapoints\n",
    "    \n",
    "    ## YOUR CODE HERE ##\n",
    "    \n",
    "    x_trn = ## YOUR CODE HERE ##\n",
    "    y_trn = ## YOUR CODE HERE ##\n",
    "    \n",
    "    x_val = ## YOUR CODE HERE ##\n",
    "    y_val = ## YOUR CODE HERE ##\n",
    "    \n",
    "    for ii in range(len(lambdas)):\n",
    "        # fit model using x_trn & predict y_val\n",
    "        # predict y_val\n",
    "        y_val_hat = ## YOUR CODE HERE ##\n",
    "        \n",
    "        # store squared error in val_mses\n",
    "        val_mses[it,ii] = ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the MSE for each lambda, averaged across the MC iterations\n",
    "\n",
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose the best lambda, print it\n",
    "best_lambda = ## YOUR CODE HERE ##\n",
    "print(\"Best lambda:\", best_lambda)\n",
    "\n",
    "# Fit a model using the whole training set and the best lambda\n",
    "beta_hat = ## YOUR CODE HERE ##\n",
    "\n",
    "# Use that model to predict y_test\n",
    "y_test_hat = ## YOUR CODE HERE ##\n",
    "\n",
    "# Compute the MSE, print it\n",
    "test_mse = ## YOUR CODE HERE ##\n",
    "\n",
    "print(\"Test MSE:\", test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - Multiple feature spaces (20 pts)\n",
    "\n",
    "Suppose you've done an experiment and measured responses to a bunch of stimuli. You've got three different hypotheses about how the stimuli might be represented in the responses. You instantiate these hypotheses as three different linearizing transforms, giving you three different sets of features that you can extract: $X_1$, $X_2$, and $X_3$ (in variables called `x1`, `x2`, and `x3`).\n",
    "\n",
    "Feature space $X_1$ has 12 features, $X_2$ has 50 features, and $X_3$ has 100 features.\n",
    "\n",
    "Note that you've recorded $m=35$ different responses (i.e. $Y$ is an $n\\times m$ matrix). Think of these as $m$ different neurons or $m$ different voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from homework_2_utils import make_data\n",
    "\n",
    "num_training = 500 # total number of datapoints in training set\n",
    "num_test = 100 # total number of datapoints in test set\n",
    "num_features = [12, 50, 100] # number of features in each feature space\n",
    "num_responses = 35 # number of responses (voxels or neurons)\n",
    "\n",
    "# This is just a bunch of constans. don't worry about what they mean for now\n",
    "combs = [[0, 3, 4, 6],\n",
    "         [1, 3, 5, 6],\n",
    "         [2, 4, 5, 6]]\n",
    "true_variances = np.array([300, 0, 1500, 250, 250, 4000, 500]).astype(float)\n",
    "total_variance = 0.3\n",
    "true_variances = true_variances / true_variances.sum() * total_variance\n",
    "noise_variance = 1 - true_variances.sum()\n",
    "P_parts = [3] * 7\n",
    "Pnoise_models = [P - np.array(P_parts)[c].sum() for P,c in zip(num_features, combs)]\n",
    "\n",
    "# Generate the data!\n",
    "[x1_total, x2_total, x3_total], Y_total = make_data(num_training, num_test, P_parts, \n",
    "                                                    num_responses, true_variances, \n",
    "                                                    noise_variance, combs, Pnoise_models, \n",
    "                                                    num_features)\n",
    "\n",
    "x1 = x1_total.T[:num_training]\n",
    "x2 = x2_total.T[:num_training]\n",
    "x3 = x3_total.T[:num_training]\n",
    "Y = Y_total[:num_training]\n",
    "\n",
    "x1_test = x1_total.T[num_training:]\n",
    "x2_test = x2_total.T[num_training:]\n",
    "x3_test = x3_total.T[num_training:]\n",
    "Y_test = Y_total[num_training:]\n",
    "\n",
    "print('x1 shape:', x1.shape)\n",
    "print('x2 shape:', x2.shape)\n",
    "print('x3 shape:', x3.shape)\n",
    "print('Y shape:', Y.shape)\n",
    "\n",
    "print('x1_test shape:', x1_test.shape)\n",
    "print('x2_test shape:', x2_test.shape)\n",
    "print('x3_test shape:', x3_test.shape)\n",
    "print('Y_test shape:', Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) - Deciding which is best (8 pts)\n",
    "\n",
    "Construct separate linear models using each of the three different feature spaces. Use those models to predict responses in the test set. Compute the prediction performance of each model. Which model is the best overall? (Don't worry about statistical comparison, just choose the one with the lowest average MSE across the responses.)\n",
    "\n",
    "You should fit a model for each feature space using ridge regression. Use cross-validation (pick your flavor) to select the best ridge parameter separately for each. It's fine here to use the same ridge parameter for each of the $m$ responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambdas = np.logspace(1, 7, 15) # use these lambdas\n",
    "\n",
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot mean validation set MSE as a function of lambda for each feature space\n",
    "\n",
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find best lambda for each feature space\n",
    "best_lambda_1 = ## YOUR CODE HERE ##\n",
    "best_lambda_2 = ## YOUR CODE HERE ##\n",
    "best_lambda_3 = ## YOUR CODE HERE ##\n",
    "print best_lambda_1, best_lambda_2, best_lambda_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit models with best lambdas and entire training set\n",
    "beta_1 = ## YOUR CODE HERE ##\n",
    "beta_2 = ## YOUR CODE HERE ##\n",
    "beta_3 = ## YOUR CODE HERE ##\n",
    "\n",
    "# Predict Y_test with each, compute MSE\n",
    "mse = lambda a, b: ((a - b)**2).mean()\n",
    "\n",
    "test_mse_1 = ## YOUR CODE HERE ##\n",
    "test_mse_2 = ## YOUR CODE HERE ##\n",
    "test_mse_3 = ## YOUR CODE HERE ##\n",
    "\n",
    "print test_mse_1, test_mse_2, test_mse_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) - Variance partitioning (12 pts)\n",
    "\n",
    "Each of these feature spaces explain something about the response. Does the variance explained by each feature space overlap with the others? Do variance partitioning to find out!\n",
    "\n",
    "There should be 7 variance partitions: one for the unique contribution of each feature space, one for the variance explained by each pair of feature spaces, and one for the variance explained by all three spaces.\n",
    "\n",
    "To get the sizes of these partitions, you'll need to fit models with each combination of feature spaces (each one alone, each pair, and all three together), then compute $R^2$ for each model, then use some simple algebra to compute the size of each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here's a function that computes R^2 of a_hat predicting a\n",
    "Rsq = lambda a, a_hat: 1 - (a - a_hat).var() / a.var()\n",
    "\n",
    "# Here it's probably useful to define a cv_ridge function that you can call a bunch of times\n",
    "def cv_ridge(x, y, x_test, y_test):\n",
    "    lambdas = np.logspace(1, 7, 15) # use these lambdas\n",
    "\n",
    "    ## YOUR CODE HERE ##\n",
    "    \n",
    "    return Rsq(y_test, y_test_hat)\n",
    "\n",
    "\n",
    "# Now fit a model with each combination of feature spaces, and compute R^2!\n",
    "# store the 7 R^2 values in all_rsqs\n",
    "# use this order for the 7 models:\n",
    "# 1. Just feature space 1\n",
    "# 2. Just 2\n",
    "# 3. Just 3\n",
    "# 4. 1 & 2\n",
    "# 5. 1 & 3\n",
    "# 6. 2 & 3\n",
    "# 7. All three (1 & 2 & 3)\n",
    "\n",
    "all_rsqs = np.zeros(7)\n",
    "\n",
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's just look at the R^2 for each of the 7 models!\n",
    "print all_rsqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Next, use the algebraic formulae to figure out the size of each partition\n",
    "# A nice way to do this is to define a matrix V such that V.dot(all_rsqs) = partition_sizes\n",
    "\n",
    "## YOUR CODE HERE ##\n",
    "\n",
    "# partition_sizes should have the size of each partition (in the same units as R^2)\n",
    "partition_sizes = ## YOUR CODE HERE ##\n",
    "\n",
    "# let's compare your partition_sizes to true_variances, which is what they should be!\n",
    "zip(partition_sizes, true_variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any of your estimates come out negative -- what happened? Can you correct them by tweaking how you're doing the cross-validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Answer what happened here._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
