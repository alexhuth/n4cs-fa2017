{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "In this homework you are going to implement and test linear model fitting functions, and data quality checking functions. You will need to install (at least) python, jupyter, matplotlib, and numpy to do this assignment. Installing anaconda is a quick way to get all of those things.\n",
    "\n",
    "There are 4 problems worth a total of 32 points. The description for each problem will tell you how many points each part is worth.\n",
    "\n",
    "You should not need to edit the boilerplate code in this notebook, but wherever you see `## YOUR CODE HERE ##`, you should replace that with your own code (obviously).\n",
    "\n",
    "To turn in your homework, email your finished `.ipynb` file to `huth@cs.utexas.edu`. This homework is due on 10/17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - Data Quality (6 pts)\n",
    "**Before you do anything else, _make sure your data is good_.**\n",
    "\n",
    "In this problem you will implement a few different methods for checking data quality. You will also answer brief questions about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make testing data\n",
    "\n",
    "def q_make_data(signal_size, n_repeats, n_timepoints):\n",
    "    signal = np.random.randn(n_timepoints)\n",
    "    data = np.random.randn(n_repeats, n_timepoints) + (signal_size ** 0.5 * signal)\n",
    "    return data\n",
    "\n",
    "q_data = q_make_data(signal_size=0.5, n_repeats=50, n_timepoints=300)\n",
    "\n",
    "# The scenario: you've done 50 repeats of the same 300-second experiment, \n",
    "# while measuring the output of 1 neuron\n",
    "# q_data is a 50 x 300 matrix with the output at each second in each of the 50 repeats\n",
    "print q_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) - Signal to Noise Ratio (SNR) (1 pt)\n",
    "\n",
    "Write code that computes the SNR of your measurement, `q_data`. Don't use the bias correction factor that we talked about in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def snr_func(data):\n",
    "    ## YOUR CODE HERE ##\n",
    "\n",
    "print('Estimated SNR:', snr_func(q_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) - Explainable Variance (EV) (1 pt)\n",
    "\n",
    "Write code that computes the EV of your measurement, `q_data`. Don't use the bias correction factor that we talked about in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ev_func(data):\n",
    "    ## YOUR CODE HERE ##\n",
    "\n",
    "print('Estimated EV:', ev_func(q_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) - Test how SNR and EV depend on the number of repetitions (3 pts)\n",
    "Run the two following cells using the `snr_func` and `ev_func` that you defined above, then answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_tests = 150\n",
    "n_repeats = np.arange(5, 45, 5)\n",
    "\n",
    "snr_estimates = np.array([[snr_func(q_make_data(signal_size=0.5, n_repeats=r, n_timepoints=300)) \n",
    "                           for _ in range(n_tests)] \n",
    "                          for r in n_repeats])\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.errorbar(n_repeats, snr_estimates.mean(1), yerr=snr_estimates.std(1))\n",
    "plt.xlabel('Number of repeats')\n",
    "plt.ylabel('Estimated SNR')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(n_repeats, snr_estimates.std(1), 'o-')\n",
    "plt.xlabel('Number of repeats')\n",
    "plt.ylabel('Standard deviation of estimated SNR')\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ev_estimates = np.array([[ev_func(q_make_data(signal_size=0.5, n_repeats=r, n_timepoints=300)) \n",
    "                          for _ in range(n_tests)] \n",
    "                         for r in n_repeats])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.errorbar(n_repeats, ev_estimates.mean(1), yerr=ev_estimates.std(1))\n",
    "plt.xlabel('Number of repeats')\n",
    "plt.ylabel('Estimated EV')\n",
    "plt.grid();\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(n_repeats, ev_estimates.std(1), 'o-')\n",
    "plt.xlabel('Number of repeats')\n",
    "plt.ylabel('Standard deviation of estimated EV')\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c - continued) Questions\n",
    "Please answer each question below by modifying this cell.\n",
    "\n",
    "#### i. We know that $data = noise + \\sqrt{0.5} * signal$. What would the SNR be if we had an infinite amount of data? Why? (1 pt)\n",
    "_Answer here._\n",
    "\n",
    "#### ii. What would the EV be if we had an infinite amount of data? (1 pt)\n",
    "_Answer here._\n",
    "\n",
    "#### ii. Our estimates of SNR and EV are biased. How does the bias change as we change the number of repeats? Why? (1 pt)\n",
    "_Answer here._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) - Test how SNR and EV depend on the number of timepoints (1 pt)\n",
    "Similarly, run the two following cells using your functions, then answer the question below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_tests = 150\n",
    "n_timepoints = np.arange(100, 500, 50)\n",
    "\n",
    "snr_estimates = np.array([[snr_func(q_make_data(signal_size=0.3, n_repeats=50, n_timepoints=t)) \n",
    "                           for _ in range(n_tests)] \n",
    "                          for t in n_timepoints])\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.errorbar(n_timepoints, snr_estimates.mean(1), yerr=snr_estimates.std(1))\n",
    "plt.xlabel('Number of timepoints')\n",
    "plt.ylabel('Estimated SNR')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(n_timepoints, snr_estimates.std(1), 'o-')\n",
    "plt.xlabel('Number of timepoints')\n",
    "plt.ylabel('Standard deviation of estimated SNR')\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ev_estimates = np.array([[ev_func(q_make_data(signal_size=0.3, n_repeats=50, n_timepoints=t)) \n",
    "                          for _ in range(n_tests)] \n",
    "                         for t in n_timepoints])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.errorbar(n_timepoints, ev_estimates.mean(1), yerr=ev_estimates.std(1))\n",
    "plt.xlabel('Number of timepoints')\n",
    "plt.ylabel('Estimated EV')\n",
    "plt.grid();\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(n_timepoints, ev_estimates.std(1), 'o-')\n",
    "plt.xlabel('Number of timepoints')\n",
    "plt.ylabel('Standard deviation of estimated EV')\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d - continued) Question\n",
    "\n",
    "#### i. Does changing the number of timepoints affect the bias or the variance in our estimates of SNR and EV? (1 pt)\n",
    "_Answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Bonus) (+3 pts)\n",
    "Suppose you have enough time to collect 15,000 total datapoints, but you can choose whether you want to collect a lot of repetitions with a short experiment, or a few repetitions with a long experiment. Let's also suppose that the shortest experiment you can do would be 10 timepoints long. How would you set `n_repeats` and `n_timepoints` to get the best estimate of SNR or EV? And how would you define _best_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert code for answering the bonus question here, if you want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - Gradient Descent (12 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make testing data\n",
    "\n",
    "def gd_make_data(nsamp=100, noise=0):    \n",
    "    # Generate a two dimensional stimulus (e.g two pixels) with correlations and 100 samples (e.g. points in time)\n",
    "    # First pixel data\n",
    "    x1 = np.random.randn(nsamp)\n",
    "\n",
    "    # Second pixel that is correlated with the first\n",
    "    x2 = .4 * x1 + .6 * np.random.randn(nsamp)\n",
    "\n",
    "    # Concatinate into a stimulus matrix - here rows are dimensions and columns are time points.\n",
    "    x = np.vstack([x1, x2])\n",
    "\n",
    "    ## Generate weights and the corresponding one dimensional response \n",
    "    # Set weights on each channel\n",
    "    b = np.array([1, 7])\n",
    "\n",
    "    # Make response of system - this is the output of our toy neuron\n",
    "    y = np.dot(x.T, b) + np.random.randn(nsamp) * noise\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "x, y = gd_make_data()\n",
    "\n",
    "# Plot timeseries\n",
    "plt.plot(x[0])\n",
    "plt.plot(x[1])\n",
    "plt.plot(y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We are going to pretend we don't know h and make a search for h values by settting up \n",
    "# a range of potential values for h1 and h2\n",
    "\n",
    "b1, b2 = np.meshgrid(np.arange(-1, 10, .2), np.arange(-1, 10, .2))\n",
    "bs = np.vstack([b1.ravel(), b2.ravel()])\n",
    "\n",
    "# get responses from each set of weights\n",
    "ys = np.dot(x.T, bs)\n",
    "\n",
    "# calculate error between the response, y, and each of the possible responses, ys.  \n",
    "errfun = np.sum((y[:,None] - ys) ** 2, 0)\n",
    "\n",
    "# reshape for plotting\n",
    "errfun = errfun.reshape(b1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## plot contour of error surface. Note the shape of the surface is angled\n",
    "# because the two variable are correlated.\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.contour(b1, b2, errfun, 50, colors='k', alpha=0.3);\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Gradient Descent (3 pts)\n",
    "One way to solve this problem is using gradient descent. Take the derivative of the error function, and then take small steps along that derivative (i.e. add the step multiplied by a small step size, `eps` to your estimate of beta).\n",
    "\n",
    "For this problem, the features are in the variable `x`, and the response is in the variable `y`.\n",
    "\n",
    "Write code that uses gradient descent to solve this problem. Store the estimated betas after each gradient step, and then plot the path that the gradient descent took (i.e. the values of all the betas along the way) on top of the error contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gradient descent!\n",
    "\n",
    "steps = 100 # how many steps to take\n",
    "eps = 0.001 # the size of each step\n",
    "\n",
    "b_est = np.array([0.0, 0.0]) # store your current estimate of beta here\n",
    "b_est_history = np.zeros([steps+1, 2]) # assume b_est_history[0] = result before you started\n",
    "\n",
    "for ii in range(steps):\n",
    "    ## YOUR CODE HERE ##\n",
    "\n",
    "\n",
    "## plot contour of error surface and your regression path\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.contour(b1, b2, errfun, 50, colors='k', alpha=0.3)\n",
    "\n",
    "## YOUR CODE HERE ##\n",
    "\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Coordinate Descent (3 pts)\n",
    "Remember that an alternative to gradient descent is coordinate descent, where you only change the one weight (out of the two betas in this problem) that had the largest derivative on each step.\n",
    "\n",
    "Write code that uses coordinate descent to solve this problem. Again, store the estimated betas after each step, and then plot the results on top of the error contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Coordinate descent!\n",
    "steps = 100 # how many steps to take\n",
    "eps = 0.001 # the size of each step\n",
    "\n",
    "b_est = np.array([0.0, 0.0])\n",
    "b_est_history = np.zeros([steps+1, 2])\n",
    "\n",
    "for ii in range(steps):\n",
    "    ## YOUR CODE HERE ##\n",
    "\n",
    "\n",
    "## plot contour of error surface and your regression path\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.contour(b1, b2, errfun, 50, colors='k', alpha=0.3)\n",
    "\n",
    "## YOUR CODE HERE ##\n",
    "\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Gradient descent with early stopping (3 pts)\n",
    "One way to regularize your regression solution is to stop doing gradient descent before you make it all the way to the bottom. This is done by checking the error after each step on a separate dataset (here called the validation set). This can be useful when the data is noisy (unlike the situation above, which was noiseless).\n",
    "\n",
    "For this problem the training features and responses (that you should use to update the weights) are in the variables `trnx` and `trny`. The validation features and responses (that you should use to test your model along the way) are in the variables `valx` and `valy`.\n",
    "\n",
    "Implement code to do gradient descent here, but now compute and store the mean squared error on both the validation and training datasets after each step. (You don't need to actually stop early, do the same number of steps as before.) \n",
    "\n",
    "Then plot the training and validation error as a function of step number. Explain the plot.\n",
    "\n",
    "Then plot the path that the gradient descent took on top of the error contours. Note that the contours are for the noiseless case, while the data you're using here is noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heldout_data = np.load('gd-heldout.npz')\n",
    "trnx = heldout_data['trnx']\n",
    "trny = heldout_data['trny']\n",
    "valx = heldout_data['valx']\n",
    "valy = heldout_data['valy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gradient descent!\n",
    "\n",
    "steps = 100\n",
    "eps = 0.001\n",
    "\n",
    "b_est = np.array([0.0, 0.0])\n",
    "b_est_history = np.zeros([steps+1, 2])\n",
    "\n",
    "trn_err_history = np.zeros([steps])\n",
    "val_err_history = np.zeros([steps])\n",
    "\n",
    "for ii in range(steps):\n",
    "    ## YOUR CODE HERE ##\n",
    "\n",
    "## plot the training and validation error as a function of step number\n",
    "plt.figure()\n",
    "## YOUR CODE HERE ##\n",
    "plt.legend();\n",
    "\n",
    "print('Best step in held-out set:', val_err_history.argmin(), 'Weights:', b_est_history[val_err_history.argmin()])\n",
    "print('Where gradient descent ended up:', b_est_history[-1])\n",
    "\n",
    "## plot the betas along the way\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.contour(b1, b2, errfun, 50, colors='k', alpha=0.3)\n",
    "\n",
    "## YOUR CODE HERE ##\n",
    "\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Explain what's going on in the error plots here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Coordinate descent with early stopping (3 pts)\n",
    "Similarly, you can do coordinate descent with early stopping (or, at least, testing on another dataset along the way). Do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Coordinate descent!\n",
    "\n",
    "steps = 100\n",
    "eps = 0.001\n",
    "\n",
    "b_est = np.array([0.0, 0.0])\n",
    "b_est_history = np.zeros([steps+1, 2])\n",
    "trn_err_history = np.zeros([steps])\n",
    "val_err_history = np.zeros([steps])\n",
    "\n",
    "for ii in range(steps):\n",
    "    ## YOUR CODE HERE ##\n",
    "\n",
    "## plot the training and validation error as a function of step number\n",
    "plt.figure()\n",
    "## YOUR CODE HERE ##\n",
    "plt.legend();\n",
    "\n",
    "print('Best step in held-out set:', val_err_history.argmin())\n",
    "print(b_est_history[val_err_history.argmin()])\n",
    "print(b_est_history[-1])\n",
    "\n",
    "## plot the beta path\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.contour(b1, b2, errfun, 50, colors='k', alpha=0.3)\n",
    "\n",
    "## YOUR CODE HERE ##\n",
    "\n",
    "plt.axis('equal');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 - Regression: ordinary least squares (OLS) and ridge on a small problem (6 pts)\n",
    "Now that you've implemented gradient descent and coordinate descent, let's do the easy versions! For this problem you'll be finding analytic solutions to the ordinary least squares (OLS) problem and the ridge problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Solve the (noiseless) 2-feature problem with OLS. (2 pts)\n",
    "Then plot the solution on top of an error contour plot.\n",
    "\n",
    "Use the features `x` and responses `y` that were defined in the noiseless problem (2a) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta_ols = ## YOUR CODE HERE ##\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.contour(b1, b2, errfun, 50, colors='k', alpha=0.3)\n",
    "## YOUR CODE HERE ##\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Solve the (noiseless) 2-feature problem with ridge (4 pts)\n",
    "Solve the regression problem using features `x` and responses `y` using ridge. Test the different ridge regularization coefficients (lambdas) given here. For each lambda, store the betas that you find. Then plot the resulting beta path on top of an error contour plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambdas = np.logspace(0, 4, 10)\n",
    "betas_ridge = np.zeros((len(lambdas), 2))\n",
    "for ii in range(len(lambdas)):\n",
    "    ## YOUR CODE HERE ##\n",
    "\n",
    "\n",
    "## Plot the ridge solutions on the error contours\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.contour(b1, b2, errfun, 50, colors='k', alpha=0.3)\n",
    "\n",
    "## YOUR CODE HERE ##\n",
    "\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 - Regression on a high-dimensional problem (8 pts)\n",
    "For the last problem you're going to do regression on a dataset with lots of features and noise as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate high-dimensional data\n",
    "\n",
    "n_features = 400 # the number of features\n",
    "n_timepoints = 600 # the number of timepoints\n",
    "n_training = 450 # the number of timepoints that we'll use for training\n",
    "noise_level = 5.0 # how much noise to add\n",
    "\n",
    "# generate the \"true\" betas, the ones that will be used to generate the data\n",
    "beta_true = np.random.randn(n_features)\n",
    "\n",
    "# generate the feature matrix, x\n",
    "# this uses a trick to make the different features in x be pretty correlated\n",
    "u,s,vh = np.linalg.svd(np.random.randn(n_timepoints, n_features), full_matrices=False)\n",
    "x_all = (u*(s**5)).dot(vh)\n",
    "x_all /= x_all.max()\n",
    "\n",
    "# generate the responses, y = x . beta + noise\n",
    "y_all = x_all.dot(beta_true) + np.random.randn(n_timepoints) * noise_level\n",
    "\n",
    "# split x and y into training part (first n_training timepoints) ..\n",
    "x = x_all[:n_training]\n",
    "y = y_all[:n_training]\n",
    "\n",
    "# .. and validation part (remaining timepoints)\n",
    "x_val = x_all[n_training:]\n",
    "y_val = y_all[n_training:]\n",
    "\n",
    "# plot y, let's see what it looks like\n",
    "plt.plot(y_all);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) How well could we possibly do at this problem? (3 pts)\n",
    "\n",
    "#### i. Define a function to compute the mean squared error (MSE) between a signal $z$ and its estimate $\\hat{z}$ (1 pt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(z, z_hat):\n",
    "    return ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. What is the minimum possible MSE on the training set and on the validation set? (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_trn_mse = ## YOUR CODE HERE ##\n",
    "best_val_mse = ## YOUR CODE HERE ##\n",
    "\n",
    "print('Best possible MSE on training set:')\n",
    "print(best_trn_mse)\n",
    "\n",
    "print('Best possible MSE on validation set:')\n",
    "print(best_val_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii. What would MSE be on the training and validation sets if all $\\beta=0$? (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betazero_trn_mse = ## YOUR CODE HERE ##\n",
    "betazero_val_mse = ## YOUR CODE HERE ##\n",
    "\n",
    "print('MSE on training set with beta=0:')\n",
    "print(betazero_trn_mse)\n",
    "\n",
    "print('MSE on validation set with beta=0:')\n",
    "print(betazero_val_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Solve the high-dimensional problem with OLS (1 pt)\n",
    "(Note that the feature matrix `x` is transposed here as compared to problem 3.)\n",
    "\n",
    "If you do this right, the training error should be _way_ better than the minimum possible. That is truly incredible. Unbelievable. Literally. And the validation error is much worse than what you would get from just guessing that $\\beta=0$. Explain why below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta_ols = ## YOUR CODE HERE ##\n",
    "\n",
    "y_hat = ## YOUR CODE HERE ##\n",
    "y_val_hat = ## YOUR CODE HERE ##\n",
    "\n",
    "print('Training MSE:', mean_squared_error(y, y_hat))\n",
    "\n",
    "print('Validation MSE:', mean_squared_error(y_val, y_val_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Explain what the heck is going on here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Solve high-dimensional problem with ridge (5 pts)\n",
    "\n",
    "#### i. Solve the regression problem $y = x \\beta$ using ridge regression. Try the different ridge coefficients (lambdas) as given here. For each lambda, store the MSE on the training dataset, the MSE on the validation dataset, and the betas. (2 pts)\n",
    "(Note again that `x` is transposed relative to problem 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-3, 5, 10) # let's check 10 lambdas between 10^-3 and 10^5. play with this range if you like\n",
    "betas_ridge = np.zeros((len(lambdas), n_features))\n",
    "trn_mse = np.zeros(len(lambdas))\n",
    "val_mse = np.zeros(len(lambdas))\n",
    "\n",
    "for ii in range(len(lambdas)):\n",
    "    ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Plot the training MSE and validation MSE as a function of lambda. Plot horizontal lines that show the theoretical minimum and maximum MSE (i.e. when beta=0) on the validation set, which you computed above. Explain what you see. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Explanation here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii. For each feature, plot its weight (beta) as a function of lambda. Put all of these on the same plot. Explain what you see. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Explanation here._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
